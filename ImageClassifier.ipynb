{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier using Keras and TensorFlow\n",
    "### Don't forget to run the following commands in order to import the proper libraries\n",
    "### You may also need to be in a separate environment in order to run TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as od\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Conda commands for installing tensorflow, pip, and keras if you don't already have them\n",
    "#conda install tensorflow\n",
    "#conda install pip\n",
    "#pip install --upgrade tensorflow==2.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "#Importing keras and tensorlow and checking the versions\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using the famous Fashion MNIST dataset in order to learn how image classification works\n",
    "### The dataset contains roughly 70,000 images of fashion items such as t-shirts, pants, shoes, etc. and the goal is to train a model to accurately classify each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bbace7e5b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading the data into tuples\n",
    "#The first tuple contains training data for both the images and labels while the second tuple contains testing data\n",
    "fashionData = keras.datasets.fashion_mnist\n",
    "(imageTrain, labelTrain), (imageTest, labelTest) = fashionData.load_data()\n",
    "\n",
    "#Showing an example of what the first image looks like\n",
    "plt.imshow(imageTrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To view the label of an image, we follow a similar approach\n",
    "#Each number represents a given category: 0 - T-Shirt/Top, 1 - Trouser, and so forth.\n",
    "labelTrain[0]\n",
    "\n",
    "#As you can see, the label for a boot is 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boot'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numbers aren't very useful so we'll add class names to the data to make it easier to read\n",
    "classNames = [\"T-Shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Boot\"]\n",
    "\n",
    "#So now, I can directly access items based on their classname\n",
    "classNames[labelTrain[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To better understand what the dataset is, instead of using an image, we can just view it as an array\n",
    "imageTrain[0]\n",
    "\n",
    "#Each image is a 28x28 grayscale image represented by pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we understand the data, it's now time to normalize the image data and come up with a training/testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To normalize our data, we will convert it all to the same scale\n",
    "imageTrainNormalized = imageTrain / 255.0 #This restricts the pixel value to be between 0 and 1\n",
    "imageTestNormalized = imageTest / 255.0\n",
    "\n",
    "#Since we know the data is only from 0 to 255, we can do this to normalize it.\n",
    "#In typical machine learning normalization processes, we would subtract each number by the mean and divide by the\n",
    "#standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.17254902, 0.49803922, 0.71372549, 0.7254902 ,\n",
       "        0.63137255, 0.47058824, 0.21568627, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16470588,\n",
       "        0.77647059, 0.98431373, 1.        , 0.98431373, 0.97647059,\n",
       "        0.96862745, 1.        , 0.98823529, 0.83921569, 0.39215686,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00784314, 0.        , 0.        , 0.91372549,\n",
       "        0.98823529, 0.92941176, 0.9372549 , 0.91764706, 0.92941176,\n",
       "        0.92156863, 0.92941176, 0.92941176, 0.99607843, 0.89019608,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00784314, 0.        , 0.        , 0.0627451 , 0.82352941,\n",
       "        0.88235294, 0.84313725, 0.68627451, 0.85098039, 0.84705882,\n",
       "        0.75686275, 0.76862745, 0.88627451, 0.86666667, 0.81960784,\n",
       "        0.19607843, 0.        , 0.        , 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.00784314,\n",
       "        0.        , 0.        , 0.78039216, 0.89803922, 0.90980392,\n",
       "        0.90196078, 0.96078431, 0.8       , 0.85882353, 0.99215686,\n",
       "        0.96078431, 0.81176471, 0.76078431, 0.8745098 , 0.90588235,\n",
       "        0.9254902 , 0.92156863, 0.        , 0.        , 0.01176471,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.        , 0.5372549 , 0.92156863, 0.8       , 0.81960784,\n",
       "        0.78823529, 0.81960784, 0.91764706, 0.74509804, 0.91764706,\n",
       "        0.85490196, 0.84313725, 0.93333333, 0.9372549 , 0.8       ,\n",
       "        0.74117647, 0.87843137, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.76078431, 0.78823529, 0.78431373, 0.81960784,\n",
       "        0.79215686, 0.75686275, 0.80392157, 0.76078431, 0.71764706,\n",
       "        0.85490196, 0.90588235, 0.77254902, 0.6745098 , 0.70980392,\n",
       "        0.75686275, 0.80392157, 0.78039216, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.83137255, 0.79607843, 0.7372549 , 0.74117647,\n",
       "        0.76862745, 0.77647059, 0.77647059, 0.78823529, 0.76862745,\n",
       "        0.85098039, 0.70196078, 0.65490196, 0.71764706, 0.85098039,\n",
       "        0.77254902, 0.79215686, 0.85882353, 0.11764706, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.13333333, 0.88235294, 0.78431373, 0.76078431, 0.74509804,\n",
       "        0.7372549 , 0.75294118, 0.76862745, 0.75294118, 0.66666667,\n",
       "        0.79215686, 0.74509804, 0.78823529, 0.76470588, 0.78431373,\n",
       "        0.78823529, 0.81960784, 0.89019608, 0.19607843, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26666667, 0.88235294, 0.82352941, 0.82745098, 0.77647059,\n",
       "        0.75294118, 0.76862745, 0.8       , 0.76862745, 0.70980392,\n",
       "        0.83137255, 0.77254902, 0.76470588, 0.75294118, 0.80784314,\n",
       "        0.8627451 , 0.82352941, 0.89803922, 0.36470588, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.43529412, 0.8745098 , 0.89019608, 0.99215686, 0.81960784,\n",
       "        0.76862745, 0.8       , 0.82745098, 0.80784314, 0.71764706,\n",
       "        0.84705882, 0.80784314, 0.82352941, 0.79607843, 0.84313725,\n",
       "        0.95686275, 0.87843137, 0.89019608, 0.58823529, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.54509804, 0.88235294, 0.87843137, 1.        , 0.79215686,\n",
       "        0.80784314, 0.83137255, 0.81960784, 0.82745098, 0.74509804,\n",
       "        0.83529412, 0.79215686, 0.81176471, 0.80784314, 0.87058824,\n",
       "        1.        , 0.90196078, 0.8627451 , 0.74509804, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.70588235, 0.88627451, 0.87843137, 1.        , 0.78039216,\n",
       "        0.8       , 0.81176471, 0.83921569, 0.83921569, 0.74509804,\n",
       "        0.84705882, 0.80784314, 0.79607843, 0.80392157, 0.85882353,\n",
       "        0.95294118, 0.87843137, 0.83921569, 0.91764706, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.88235294, 0.8745098 , 0.89411765, 0.99607843, 0.81960784,\n",
       "        0.80784314, 0.81568627, 0.83529412, 0.82352941, 0.74901961,\n",
       "        0.84313725, 0.81176471, 0.8       , 0.81568627, 0.82745098,\n",
       "        0.97647059, 0.88627451, 0.83921569, 1.        , 0.14901961,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.98039216, 0.90980392, 0.94117647, 0.9372549 , 0.82745098,\n",
       "        0.79607843, 0.81960784, 0.80392157, 0.82745098, 0.77254902,\n",
       "        0.84313725, 0.81568627, 0.81568627, 0.83921569, 0.83529412,\n",
       "        0.9372549 , 0.90588235, 0.85882353, 1.        , 0.31764706,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.97254902, 0.9254902 , 0.96862745, 0.94117647, 0.79607843,\n",
       "        0.78431373, 0.81568627, 0.80784314, 0.83921569, 0.75686275,\n",
       "        0.83529412, 0.83137255, 0.81568627, 0.83137255, 0.82745098,\n",
       "        0.95294118, 0.94901961, 0.88235294, 0.99607843, 0.25882353,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.96862745, 0.90196078, 0.98823529, 0.88627451, 0.78039216,\n",
       "        0.82745098, 0.79215686, 0.82745098, 0.83529412, 0.71372549,\n",
       "        0.83529412, 0.83137255, 0.80784314, 0.79215686, 0.85882353,\n",
       "        0.81176471, 0.96862745, 0.87058824, 0.92941176, 0.40784314,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.03921569,\n",
       "        0.95686275, 0.85882353, 0.98039216, 0.80392157, 0.78039216,\n",
       "        0.81960784, 0.79215686, 0.81960784, 0.82745098, 0.74117647,\n",
       "        0.83921569, 0.80784314, 0.82352941, 0.78431373, 0.83137255,\n",
       "        0.60392157, 0.94117647, 0.81568627, 0.85882353, 0.54901961,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
       "        1.        , 0.87058824, 0.93333333, 0.72156863, 0.82352941,\n",
       "        0.75294118, 0.80784314, 0.81960784, 0.82352941, 0.74117647,\n",
       "        0.83529412, 0.82745098, 0.81960784, 0.75294118, 0.89411765,\n",
       "        0.60784314, 0.88627451, 0.93333333, 0.94509804, 0.65098039,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.14509804,\n",
       "        0.96078431, 0.88627451, 0.94509804, 0.58823529, 0.77254902,\n",
       "        0.74117647, 0.8       , 0.81960784, 0.82352941, 0.71764706,\n",
       "        0.83529412, 0.83529412, 0.78823529, 0.72156863, 0.84313725,\n",
       "        0.57254902, 0.84705882, 0.9254902 , 0.88235294, 0.60392157,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.22745098,\n",
       "        0.9372549 , 0.89019608, 1.        , 0.61960784, 0.75686275,\n",
       "        0.76470588, 0.8       , 0.81960784, 0.83529412, 0.70588235,\n",
       "        0.81176471, 0.85098039, 0.78039216, 0.76078431, 0.82745098,\n",
       "        0.61960784, 0.85882353, 0.9254902 , 0.84705882, 0.59215686,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.26666667,\n",
       "        0.91372549, 0.88627451, 0.95294118, 0.54509804, 0.78431373,\n",
       "        0.75686275, 0.80392157, 0.82352941, 0.81568627, 0.70588235,\n",
       "        0.80392157, 0.83137255, 0.79607843, 0.76862745, 0.84705882,\n",
       "        0.61568627, 0.70196078, 1.        , 0.84705882, 0.60784314,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.31764706,\n",
       "        0.88235294, 0.87843137, 0.82745098, 0.54117647, 0.85882353,\n",
       "        0.7254902 , 0.78823529, 0.83529412, 0.81176471, 0.77254902,\n",
       "        0.88627451, 0.83137255, 0.78431373, 0.74509804, 0.84313725,\n",
       "        0.71764706, 0.35294118, 1.        , 0.82745098, 0.57647059,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.35686275,\n",
       "        0.82352941, 0.90196078, 0.61960784, 0.44705882, 0.80392157,\n",
       "        0.73333333, 0.81568627, 0.81960784, 0.80784314, 0.75686275,\n",
       "        0.82352941, 0.82745098, 0.8       , 0.76470588, 0.8       ,\n",
       "        0.70980392, 0.09019608, 1.        , 0.83529412, 0.61960784,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.34117647,\n",
       "        0.80392157, 0.90980392, 0.42745098, 0.64313725, 1.        ,\n",
       "        0.83921569, 0.87843137, 0.87058824, 0.82352941, 0.77254902,\n",
       "        0.83921569, 0.88235294, 0.87058824, 0.82745098, 0.8627451 ,\n",
       "        0.85098039, 0.        , 0.91764706, 0.84705882, 0.6627451 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.36078431,\n",
       "        0.83529412, 0.90980392, 0.57254902, 0.01960784, 0.5254902 ,\n",
       "        0.59215686, 0.63529412, 0.66666667, 0.71764706, 0.71372549,\n",
       "        0.64313725, 0.65098039, 0.69803922, 0.63529412, 0.61176471,\n",
       "        0.38431373, 0.        , 0.94117647, 0.88235294, 0.82352941,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.16862745,\n",
       "        0.64313725, 0.80784314, 0.55294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.49803922, 0.49019608, 0.29803922,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#When we loaded the data earlier, we had 60,000 images for training and 10,000 for testing.\n",
    "#Now, we further split the training data into training/validation sets in order to tune our hyperparameters\n",
    "\n",
    "#Splitting the image data and label data into training/validation sets by choosing the first 5000 as the validation set\n",
    "#and the rest for training\n",
    "imageTrain, imageValidation = imageTrainNormalized[5000:], imageTrainNormalized[:5000]\n",
    "labelTrain, labelValidation = labelTrain[5000:], labelTrain[:5000]\n",
    "imageTest = imageTestNormalized\n",
    "\n",
    "#Showing what our new data looks like\n",
    "imageTrain[0]\n",
    "\n",
    "#You can now see that the values are all between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the data is in proper format, we can now create the neural network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#There are two types of API's in Keras to create models: Sequential and Functional\n",
    "#For simple neural networks, sequential is the most straight forward\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#Now we'll add the input layer, a couple hidden layers, and then the output layer\n",
    "#The input layer is a 28x28 array and we want the output to be one of 10 answers (clothing categories)\n",
    "model.add(keras.layers.Flatten(input_shape = [28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\")) #Relu is the preferred activation method for classification\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\")) #Softmax is the preferred activation method for \n",
    "                                                          #classification with mutually exclusive categories\n",
    "    \n",
    "#Printing a summary of our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7149 - accuracy: 0.7659 - val_loss: 0.4958 - val_accuracy: 0.8360\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4850 - accuracy: 0.8322 - val_loss: 0.4369 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4420 - accuracy: 0.8456 - val_loss: 0.4100 - val_accuracy: 0.8590\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4148 - accuracy: 0.8539 - val_loss: 0.4058 - val_accuracy: 0.8578\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3952 - accuracy: 0.8607 - val_loss: 0.3874 - val_accuracy: 0.8662\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3785 - accuracy: 0.8668 - val_loss: 0.3740 - val_accuracy: 0.8708\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3662 - accuracy: 0.8706 - val_loss: 0.3751 - val_accuracy: 0.8678\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3535 - accuracy: 0.8746 - val_loss: 0.3820 - val_accuracy: 0.8660\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3443 - accuracy: 0.8779 - val_loss: 0.3657 - val_accuracy: 0.8712\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3347 - accuracy: 0.8807 - val_loss: 0.3456 - val_accuracy: 0.8778\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3259 - accuracy: 0.8836 - val_loss: 0.3533 - val_accuracy: 0.8750\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3182 - accuracy: 0.8865 - val_loss: 0.3480 - val_accuracy: 0.8724\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3107 - accuracy: 0.8890 - val_loss: 0.3343 - val_accuracy: 0.8820\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3043 - accuracy: 0.8906 - val_loss: 0.3264 - val_accuracy: 0.8848\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2981 - accuracy: 0.8935 - val_loss: 0.3308 - val_accuracy: 0.8826\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2914 - accuracy: 0.8948 - val_loss: 0.3232 - val_accuracy: 0.8824\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2854 - accuracy: 0.8980 - val_loss: 0.3199 - val_accuracy: 0.8864\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2802 - accuracy: 0.8989 - val_loss: 0.3079 - val_accuracy: 0.8880\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2749 - accuracy: 0.9012 - val_loss: 0.3137 - val_accuracy: 0.8852\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2698 - accuracy: 0.9030 - val_loss: 0.3105 - val_accuracy: 0.8920\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2643 - accuracy: 0.9046 - val_loss: 0.3091 - val_accuracy: 0.8888\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2588 - accuracy: 0.9065 - val_loss: 0.3010 - val_accuracy: 0.8914\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2559 - accuracy: 0.9082 - val_loss: 0.3078 - val_accuracy: 0.8934\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2506 - accuracy: 0.9101 - val_loss: 0.2944 - val_accuracy: 0.8948\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2470 - accuracy: 0.9102 - val_loss: 0.2978 - val_accuracy: 0.8922\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2422 - accuracy: 0.9128 - val_loss: 0.3090 - val_accuracy: 0.8928\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2379 - accuracy: 0.9137 - val_loss: 0.3076 - val_accuracy: 0.8914\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2337 - accuracy: 0.9151 - val_loss: 0.2886 - val_accuracy: 0.8964\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2289 - accuracy: 0.9179 - val_loss: 0.3020 - val_accuracy: 0.8908\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2260 - accuracy: 0.9181 - val_loss: 0.3155 - val_accuracy: 0.8848\n"
     ]
    }
   ],
   "source": [
    "#We'll now compile our model to specify our loss function, the optimizer, and what metrics to use\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])\n",
    "#We use that specific loss function since we have labels for our categories\n",
    "#SGD is a form of gradient descent and this tells our model to use back-propagation\n",
    "\n",
    "#Next, we fit our model using the image and label training data (this could take a while based on your epoch value)\n",
    "modelHistory = model.fit(imageTrain, labelTrain, epochs = 30, validation_data = (imageValidation, labelValidation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7148729562759399,\n",
       "  0.4850018322467804,\n",
       "  0.44202595949172974,\n",
       "  0.4148392081260681,\n",
       "  0.3952029347419739,\n",
       "  0.37852248549461365,\n",
       "  0.366193562746048,\n",
       "  0.3535443842411041,\n",
       "  0.34428277611732483,\n",
       "  0.33467987179756165,\n",
       "  0.32588836550712585,\n",
       "  0.31816720962524414,\n",
       "  0.31069913506507874,\n",
       "  0.30433014035224915,\n",
       "  0.2980756163597107,\n",
       "  0.2914330065250397,\n",
       "  0.2854008078575134,\n",
       "  0.2802108824253082,\n",
       "  0.27494674921035767,\n",
       "  0.26983264088630676,\n",
       "  0.26425063610076904,\n",
       "  0.2587514817714691,\n",
       "  0.2558557391166687,\n",
       "  0.2505996525287628,\n",
       "  0.24698257446289062,\n",
       "  0.24224479496479034,\n",
       "  0.2378620207309723,\n",
       "  0.23369325697422028,\n",
       "  0.22888478636741638,\n",
       "  0.22596612572669983],\n",
       " 'accuracy': [0.7658727169036865,\n",
       "  0.8322181701660156,\n",
       "  0.8455818295478821,\n",
       "  0.8538908958435059,\n",
       "  0.8606908917427063,\n",
       "  0.8668000102043152,\n",
       "  0.8706181645393372,\n",
       "  0.8745818138122559,\n",
       "  0.8778908848762512,\n",
       "  0.8806727528572083,\n",
       "  0.8835999965667725,\n",
       "  0.886509120464325,\n",
       "  0.8889999985694885,\n",
       "  0.8905818462371826,\n",
       "  0.8934727311134338,\n",
       "  0.8948181867599487,\n",
       "  0.8980363607406616,\n",
       "  0.8989272713661194,\n",
       "  0.901163637638092,\n",
       "  0.902999997138977,\n",
       "  0.9046000242233276,\n",
       "  0.9065272808074951,\n",
       "  0.9082363843917847,\n",
       "  0.9100909233093262,\n",
       "  0.9101999998092651,\n",
       "  0.912818193435669,\n",
       "  0.9137272834777832,\n",
       "  0.9151272773742676,\n",
       "  0.9178727269172668,\n",
       "  0.9181272983551025],\n",
       " 'val_loss': [0.4957599639892578,\n",
       "  0.4369388222694397,\n",
       "  0.4100295901298523,\n",
       "  0.40578052401542664,\n",
       "  0.3873842656612396,\n",
       "  0.37403231859207153,\n",
       "  0.3751482665538788,\n",
       "  0.3820037245750427,\n",
       "  0.36571159958839417,\n",
       "  0.3455609083175659,\n",
       "  0.35328760743141174,\n",
       "  0.3479912281036377,\n",
       "  0.33425450325012207,\n",
       "  0.3264244496822357,\n",
       "  0.33079084753990173,\n",
       "  0.3232404589653015,\n",
       "  0.3198585510253906,\n",
       "  0.30787423253059387,\n",
       "  0.31374049186706543,\n",
       "  0.31049370765686035,\n",
       "  0.3091071844100952,\n",
       "  0.3010357916355133,\n",
       "  0.30782678723335266,\n",
       "  0.2943946421146393,\n",
       "  0.2977844476699829,\n",
       "  0.30896925926208496,\n",
       "  0.30757755041122437,\n",
       "  0.28859743475914,\n",
       "  0.3020150661468506,\n",
       "  0.31552156805992126],\n",
       " 'val_accuracy': [0.8360000252723694,\n",
       "  0.8500000238418579,\n",
       "  0.859000027179718,\n",
       "  0.8578000068664551,\n",
       "  0.8661999702453613,\n",
       "  0.8708000183105469,\n",
       "  0.8677999973297119,\n",
       "  0.8659999966621399,\n",
       "  0.8712000250816345,\n",
       "  0.8777999877929688,\n",
       "  0.875,\n",
       "  0.8723999857902527,\n",
       "  0.8820000290870667,\n",
       "  0.8848000168800354,\n",
       "  0.8826000094413757,\n",
       "  0.8823999762535095,\n",
       "  0.8863999843597412,\n",
       "  0.8880000114440918,\n",
       "  0.885200023651123,\n",
       "  0.8920000195503235,\n",
       "  0.8888000249862671,\n",
       "  0.8913999795913696,\n",
       "  0.8934000134468079,\n",
       "  0.8948000073432922,\n",
       "  0.8921999931335449,\n",
       "  0.892799973487854,\n",
       "  0.8913999795913696,\n",
       "  0.896399974822998,\n",
       "  0.8907999992370605,\n",
       "  0.8848000168800354]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see some attributes of our model, there are a few functions we can call to get a better idea\n",
    "modelHistory.history\n",
    "\n",
    "#The history function lets us see how our model improved after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once the model has been compiled and ran, it's now time to evaluate how it performed.\n",
    "### We will also show how to make predictions using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3565 - accuracy: 0.8713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35647693276405334, 0.8712999820709229]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To evaluate our model, we simply use the evaluate function along with our test set to compare \n",
    "model.evaluate(imageTest, labelTest)\n",
    "\n",
    "#The first number is the loss and the second number is the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To make predictions of what category an image will fall in based on it's characteristics, we'll just take a sample\n",
    "#of the data and use the predict function\n",
    "imageSampleSet = imageTest[:3]\n",
    "\n",
    "#First, we'll find out the probabibility that the image belongs to each class\n",
    "classProbability = model.predict(imageSampleSet)\n",
    "classProbability.round(2)\n",
    "\n",
    "#As you can see, for the first image, it's 99% sure it's the 10th class (or a boot). And in the next samlple,\n",
    "#it's certain that it belongs to the 3rd class (or a Pullover) and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-b4fce26441c2>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To just only predict what class the image belongs to, we use the predict_classes function\n",
    "classPrediction = model.predict_classes(imageSampleSet)\n",
    "classPrediction\n",
    "\n",
    "#As you can see, it chose 9, 2, and 1 (if you go back up to the array at the beginning of this, you can see what each\n",
    "#number corresponds to)\n",
    "#Also, don't forget arrays start at 0, so 9 actually is the 10th item in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To make things more obvious, we'll just use the classNames variable we created earlier\n",
    "np.array(classNames)[classPrediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you'd like to save your model so you don't have to train it everytime you close Jupyter, \n",
    "#run the following commands but with your own specifications\n",
    "model.save('ImageClassificationModel.h5') #It saves in current directory\n",
    "\n",
    "#If you want to change your current directory, just type %cd \"insert directory here\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
